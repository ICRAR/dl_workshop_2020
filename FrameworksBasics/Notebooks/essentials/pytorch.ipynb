{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch\n",
    "\n",
    "One of the most popular frameworks (see also chainer). It supports (python) imperative programming. It is also possible to export models and import them in C++ code for efficiency (production code). The C++ API is super easy to learn. Extremely user friendly, most cited in recent years for computer vision and nlp research. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources \n",
    "#### Tutorials\n",
    "    1. https://pytorch.org/tutorials/\n",
    "    2. fast.ai # tutorials built on pytorch\n",
    "\n",
    "#### Awesome pytorch\n",
    "    1. Just google \"awesome pytorch\", extreme support over the internet \n",
    "    2. https://github.com/eriklindernoren/PyTorch-GAN #Nice GAN implementations\n",
    "#### Community (forum)\n",
    "    1. https://discuss.pytorch.org/\n",
    "    2. https://forums.fast.ai/ # fastai forum, excellent resource\n",
    "#### API Reference\n",
    "    1. https://pytorch.org/docs/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array creation routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [5, 6, 7]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(((1,2,3),(5,6,7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones((2,3))\n",
    "print (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2710, -0.5871,  0.7785],\n",
      "        [ 0.3618, -0.5209,  0.1836]])\n"
     ]
    }
   ],
   "source": [
    "# torch.rand((2,3)) #returns a uniform in [0,1]\n",
    "# torch.randn((2,3)) #returns a normal of shape(2,3) (mu=0,sigma=1)\n",
    "y = torch.Tensor(2,3).uniform_(-1.,1.) # inplace creation \"_\", avoids unnecessary copying\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.full((2,3), 2.0)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), torch.Size([2, 3]), torch.float32, device(type='cpu'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, x.size(),x.dtype,x.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5420, -1.1742,  1.5570],\n",
       "        [ 0.7236, -1.0419,  0.3672]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3944, 2.1511, 1.6545],\n",
       "        [1.7295, 2.3431, 1.0858]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5458, 1.1631, 0.7078],\n",
       "        [0.7630, 1.4093, 0.4008]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.6000, -1.6832],\n",
       "         [-1.6000, -1.6832]]), tensor(24.))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.dot applies only on 1D vectors\n",
    "torch.mm(x,y.transpose(0,1)), torch.dot(x.view(-1),x.view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing/slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.9142)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1511, -0.3455],\n",
       "        [ 0.3431, -0.9142]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:,1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6056,  2.0000,  2.0000],\n",
      "        [-0.2705,  2.0000,  2.0000]])\n"
     ]
    }
   ],
   "source": [
    "y[:,1:3]=2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6056,  2.0000,  2.0000]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:1,:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting\n",
    "\n",
    "Operating between two arrays (tensors) of different dimensionality:\n",
    "1. Follows numpy semantics\n",
    "2. They must have same rank\n",
    "3. The dimension that has value 1 is repeated along that axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "x =  tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "-----------------\n",
      "y =  tensor([[0., 1., 2.]])\n",
      "-----------------\n",
      "y.shape=  torch.Size([1, 3])\n",
      "-----------------\n",
      "x + y =  tensor([[1., 2., 3.],\n",
      "        [1., 2., 3.],\n",
      "        [1., 2., 3.]])\n",
      "-----------------\n",
      "x * y =  tensor([[0., 1., 2.],\n",
      "        [0., 1., 2.],\n",
      "        [0., 1., 2.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3,3)\n",
    "print (\"-----------------\")\n",
    "print('x = ', x)\n",
    "print (\"-----------------\")\n",
    "y = torch.tensor([[0.,1.,2.]])\n",
    "print('y = ', y)\n",
    "print (\"-----------------\")\n",
    "print('y.shape= ',y.shape)\n",
    "print (\"-----------------\")\n",
    "print('x + y = ', x + y)\n",
    "print (\"-----------------\")\n",
    "print('x * y = ', x * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make explicit the dimension you want to broadcast to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "x =  tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "-----------------\n",
      "y =  tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "-----------------\n",
      "y.shape=  torch.Size([3, 1])\n",
      "-----------------\n",
      "x + y =  tensor([[1., 1., 1.],\n",
      "        [2., 2., 2.],\n",
      "        [3., 3., 3.]])\n",
      "-----------------\n",
      "x * y =  tensor([[0., 0., 0.],\n",
      "        [1., 1., 1.],\n",
      "        [2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3,3)\n",
    "print (\"-----------------\")\n",
    "print('x = ', x)\n",
    "print (\"-----------------\")\n",
    "y = torch.tensor([[0.],[1.],[2.]])\n",
    "print('y = ', y)\n",
    "print (\"-----------------\")\n",
    "print('y.shape= ',y.shape)\n",
    "print (\"-----------------\")\n",
    "print('x + y = ', x + y)\n",
    "print (\"-----------------\")\n",
    "print('x * y = ', x * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back and forth to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "a = x.numpy() # misses the \"as\"\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(a)\n",
    "# x = torch.from_numpy(a) # more efficient method for creating torch.Tensor objects\n",
    "print (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing device (context) - CPU/GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find out how many GPUs exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# This shows if there are GPUs available in your system \n",
    "print (torch.cuda.is_available()) # prints True if GPU exists on your system and IS recognized (did u install correctly?)\n",
    "\n",
    "# This the cound of available gpu devices\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], device='cuda:0')\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# If you have a gpu available\n",
    "a = torch.ones((2,2)) # default creation on cpu (ram memory)\n",
    "\n",
    "if torch.cuda.is_available(): \n",
    "    a = a.to(torch.device('cuda'))\n",
    "    #a = a.to(torch.device('cuda:0'))\n",
    "    \n",
    "# Alternative creation routines\n",
    "# a = torch.ones((2,2)).cuda() # equivalent definition\n",
    "# a = torch.ones(2,2).cuda(0) # here 0 represents the index of the gpu device\n",
    "print (a)\n",
    "print (a.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative ways to copy a cpu tensor to gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9078, 0.5974, 0.0684],\n",
      "        [0.1241, 0.4069, 0.1941],\n",
      "        [0.3595, 0.5783, 0.7154]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "a = a.cuda()\n",
    "a = a.to(torch.device('cuda'))\n",
    "\n",
    "# recommended 1:\n",
    "if torch.cuda.is_available():\n",
    "    a = a.cuda()\n",
    "    \n",
    "# recommended 2, define global device variable (avoids multiple if checks):\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "c = torch.rand(3,3)\n",
    "c = c.to(device)\n",
    "print (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to perform operations between tensors (vectors), both of them must live in the same device (context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4699, 0.4209, 0.9168],\n",
      "        [0.8787, 0.8552, 0.9404],\n",
      "        [0.5054, 0.7507, 0.9644]])\n",
      "cpu\n",
      "tensor([[0.4838, 0.5177, 0.4177],\n",
      "        [0.9930, 0.1040, 0.5883],\n",
      "        [0.5516, 0.3947, 0.0138]], device='cuda:0')\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand((3,3)) # lives on CPU (default)\n",
    "# a = torch.rand(3,3,device=torch.device('cpu')) # alternative definition, lives on cpu\n",
    "print (a)\n",
    "print(a.device)\n",
    "b = torch.rand(3,3,device=torch.device('cuda')) # lives on GPU\n",
    "# alternative definitions\n",
    "#b = torch.rand(3,3).cuda()\n",
    "#b = torch.rand(3,3).to(torch.device('cuda'))\n",
    "print (b)\n",
    "print (b.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9538, 0.9385, 1.3345],\n",
      "        [1.8717, 0.9592, 1.5287],\n",
      "        [1.0571, 1.1454, 0.9782]], device='cuda:0')\n",
      "tensor([[0.9538, 0.9385, 1.3345],\n",
      "        [1.8717, 0.9592, 1.5287],\n",
      "        [1.0571, 1.1454, 0.9782]])\n"
     ]
    }
   ],
   "source": [
    "# Correct operation \n",
    "print (a.cuda()+b) # copy a to G-pu, result lives on GPU\n",
    "print (a+b.cpu()) # copy b to C-pu, result lives on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected type torch.FloatTensor but got torch.cuda.FloatTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-6587dbae3b51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: expected type torch.FloatTensor but got torch.cuda.FloatTensor"
     ]
    }
   ],
   "source": [
    "# to err is to learn\n",
    "a = a.to(torch.device('cpu'))\n",
    "b = b.to(torch.device('cuda'))\n",
    "print (a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some basic linear algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a as an array:=  tensor([3])\n",
      "This is a as a float:=  3\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([3])\n",
    "print (\"This is a as an array:= \", a)\n",
    "a = a.item()\n",
    "print (\"This is a as a float:= \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## matrix - vector product (not broadcasting!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=  tensor([[0.0187, 0.6996, 0.3053, 0.9990, 0.9009],\n",
      "        [0.7379, 0.9380, 0.5124, 0.4761, 0.7619],\n",
      "        [0.2273, 0.0469, 0.7664, 0.3275, 0.0321]])\n",
      "b=  tensor([[0.3260],\n",
      "        [0.8682],\n",
      "        [0.4630],\n",
      "        [0.1604],\n",
      "        [0.1782]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3,5)\n",
    "print (\"a= \",a)\n",
    "b = torch.rand(5,1) # Compare with declaring explicitely \n",
    "print (\"b= \",b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:=  tensor([[1.0756],\n",
      "        [1.5043],\n",
      "        [0.5279]])\n",
      "shape of output matrix:  torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "# mm: matrix multiplication\n",
    "result = torch.mm(a,b)\n",
    "print (\"result:= \",result)\n",
    "print (\"shape of output matrix: \", result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**: in pytorch dot operation applies only to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.b:=  tensor(0.5438)\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3)\n",
    "b = torch.rand(3)\n",
    "result = torch.dot(a,b)\n",
    "print (\"a.b:= \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with declaring matrix b only with its first dimension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial b.shape=  torch.Size([5, 1])\n",
      "new shape of b:  torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3,5)\n",
    "#print (\"a= \",a)\n",
    "b = torch.rand(5,1) # Compare with declaring explicitely \n",
    "#print (\"b= \",b)\n",
    "\n",
    "print (\"initial b.shape= \", b.shape)\n",
    "b = torch.squeeze(b) # just like np.squeeze - removes the redundant dimension\n",
    "print (\"new shape of b: \", b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-be83bae2ff1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# In pytorch the operator mm does not allow matrix-vector product\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"result= \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"shape of output matrix: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "# In pytorch the operator mm does not allow matrix-vector product\n",
    "result = torch.mm(a,b)\n",
    "print (\"result= \", result)\n",
    "print (\"shape of output matrix: \", result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: A handy operation in pytorch is torch.tensor.view, to change the shape of a tensor/layer/Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.shape before:=  torch.Size([3, 5])\n",
      "a.shape after:=  torch.Size([15])\n"
     ]
    }
   ],
   "source": [
    "print (\"a.shape before:= \", a.shape)\n",
    "a = a.view(-1) # equivalent to np.flatten, does not copy elements\n",
    "print (\"a.shape after:= \",a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation\n",
    "\n",
    "We train neural networks with (modified versions of) gradient descent. Therefore we need a mechanism that automatically evaluates derivatives for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([0.,1.,2.,3.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "print (x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x.grad stores the value of the derivatives of functions that take x as input, with respect to x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to explicitely declare we require gradient evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print (x.requires_grad )\n",
    "x.requires_grad = True\n",
    "print (x.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in pytorch we do not tell explicitely to record the computation graph, it is done automatically.\n",
    "The backwarad function applies only to scalar (i.e. not tensor/array) objects. So we need to evaluate a scalar loss before calling it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-ff25dc7847d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This doesn't work, because y is not a scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "y = x**2\n",
    "y.backward() # This doesn't work, because y is not a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.dot(x,x) # This is a scalar, (x1^2+x2^2+x3^2+x4^2)\n",
    "y.backward() # now this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 2., 4., 6.]) tensor([0., 2., 4., 6.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# The derivative is 2*x_i, for each index i=1,...,4\n",
    "print (x.grad, 2*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bit more complex - hey, did you zero the gradients?\n",
    "z = x**2\n",
    "y = torch.sum(z*z)\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0.,   6.,  36., 114.]) tensor([  0.,   4.,  32., 108.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print (x.grad,4*x**3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHAT JUST HAPPENED?\n",
    "\n",
    "In pytorch, by default, when a new gradient computation is called (f(x).backward()) the new gradients are added in place to the old gradients. We need to explicitly zero them before a new backward computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0.,   4.,  32., 108.]) tensor([  0.,   4.,  32., 108.], grad_fn=<MulBackward0>)\n",
      "tensor([  0.,   8.,  64., 216.]) tensor([  0.,   4.,  32., 108.], grad_fn=<MulBackward0>)\n",
      "tensor([  0.,  12.,  96., 324.]) tensor([  0.,   4.,  32., 108.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Without zero grad, dy/dx != 4*x^3\n",
    "x = torch.tensor([0,1.,2.,3])\n",
    "x.requires_grad=True\n",
    "for _ in range(3):\n",
    "    z = x**2\n",
    "    y = torch.sum(z*z)\n",
    "    y.backward()\n",
    "    print (x.grad,4*x**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0.,   4.,  32., 108.]) tensor([  0.,   4.,  32., 108.], grad_fn=<MulBackward0>)\n",
      "tensor([  0.,   4.,  32., 108.]) tensor([  0.,   4.,  32., 108.], grad_fn=<MulBackward0>)\n",
      "tensor([  0.,   4.,  32., 108.]) tensor([  0.,   4.,  32., 108.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# With correct zero grad, dy/dx = 4*x^3\n",
    "x = torch.tensor([0,1.,2.,3])\n",
    "x.requires_grad=True\n",
    "for _ in range(3):\n",
    "    z = x**2\n",
    "    y = torch.sum(z*z)\n",
    "    y.backward()\n",
    "    print (x.grad,4*x**3)\n",
    "    x.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggested reading: autograd: \n",
    "https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
